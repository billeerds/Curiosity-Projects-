---
title: "Inaugural Address Text Analysis"
author: "William Eerdmans"
date: "March 15, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr) 
library(dplyr)
library(corrplot)
# Here are the documentation for packages used in this code:
#https://cran.r-project.org/web/packages/tm/tm.pdf
library(tm)
#https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf
library(topicmodels)

# Use the SnowballC package to do stemming.
library(SnowballC) 
library(ggplot2)
library("wordcloud")
library("RColorBrewer")
library(quanteda)
library(syuzhet)
```

```{r, cache=TRUE} 
dirname <- file.path("C:/Users/wjeer/OneDrive/Side Projects/Presidential Inagural Addresses/Addresses")
docs <- Corpus(DirSource(dirname, encoding = "UTF-8"))
docs[[1]]$content[1]
# The following steps pre-process the raw text documents. 
# Remove punctuations and numbers because they are generally uninformative. 
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)

# Convert all words to lowercase. 
docs <- tm_map(docs, content_transformer(tolower))

# Remove stopwords such as "a", "the", etc. 
docs <- tm_map(docs, removeWords, stopwords("english"))

# Use the SnowballC package to do stemming. 
docs <- tm_map(docs, stemDocument)

# Remove excess white spaces between words. 
docs <- tm_map(docs, stripWhitespace)

# You can inspect the first document to see what it looks like with 
docs[[1]]$content[1]

# Convert all documents to a term frequency matrix. 
tfm <- DocumentTermMatrix(docs)

# We can check the dimension of this matrix by calling dim() 
print(dim(tfm))
```

***Find initial overall frequency of speechs, words used the most, etc.***
```{r}
#Barplot of top 10 most frequent words
#initial strategy @ http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

m <- as.matrix(tfm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

barplot(rev(d[2:26,]$freq), las = 2, names.arg = rev(d[2:26,]$word),
        horiz=TRUE,
        col ="lightblue", main ="Most frequent words across addresses",
        xlab = "Word frequencies",
        cex.names=1)

ggplot(d[2:26,], aes(x=reorder(word, freq), y=freq, fill = log(freq))) + geom_bar(stat="identity") + coord_flip() + ylab("Word Occurances") + xlab("") + theme(axis.text = element_text(size=20), axis.title=element_text(size=20), title=element_text(size=20))+ guides(fill=FALSE) +ggtitle("Top 25 words used in Inaugural Addresses")


#Create word cloud or chart of most used words, removed will
wordcloud(words = d$word[-c(d$word == "will")], freq = d$freq, min.freq = 100,
          max.words=500, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "RdBu"))
```

***In this section find the most used words by President and unique words***
```{r}
#Find the total words per speech after transformation
all_words <- rowSums(as.matrix(tfm))
tot_words <- data.frame(Last.Name = gsub('.{4}$', '', names(all_words)), Word_Frequency=all_words)

#Create bar chart of presidents and words
ggplot(tot_words, aes(x = reorder(Last.Name, -Word_Frequency), y = Word_Frequency, fill = log(Word_Frequency))) + geom_bar(stat = "identity") + labs(x = "President") + theme(axis.text.x = element_text(angle = 60, hjust = 1), text = element_text(size=20)) + ggtitle("Total words after transformation") + guides(fill=FALSE) + ylab("Number of words used")

#What about unique words?
a <- as.matrix(tfm)

#Make all duplicates just 1 (binary)
a[a>0] <- 1
unique_words <- rowSums(a)
uniq_words <- data.frame(Last.Name = gsub('.{4}$', '', names(unique_words)), Word_Frequency_Unique=unique_words)

#Create bar chart of presidents and words
ggplot(uniq_words, aes(x = reorder(Last.Name, -Word_Frequency_Unique), y = Word_Frequency_Unique, fill = log(Word_Frequency_Unique))) + geom_bar(stat = "identity") + labs(x = "President") + theme(text = element_text(size=20), axis.text.x = element_text(angle = 60, hjust = 1)) + ggtitle("Unique words used in speech") + guides(fill=FALSE) + ylab("Unique words used")

#Bring in the Date of Inauguration and merge the two frequency dataframes to it
#Create time series graph showing number of words of speeches over time (just interesting, haha)
pres_dates <- read.csv("President_year.csv")
totWords_dates <- merge(tot_words, pres_dates, by = "Last.Name")
word_freq_dates <- merge(totWords_dates, uniq_words, by = "Last.Name" )

#create time series
ggplot(word_freq_dates) + geom_line(aes(x=start_year, y = Word_Frequency), color="blue", size = 1) + geom_line(aes(x=start_year, y = Word_Frequency_Unique), color = "red", size = 1) + geom_text(aes(x = start_year, y = Word_Frequency, label = Last.Name, group = 1), angle = 45, check_overlap = TRUE) + theme(text = element_text(size=20)) + ylab("Number of words") + xlab("Year")

#Unique word to total word ratio
word_freq_dates$uniq_ratio <- word_freq_dates$Word_Frequency_Unique / word_freq_dates$Word_Frequency

#create unique ratio graph
ggplot(word_freq_dates, aes(x = start_year, y = uniq_ratio,label = Last.Name)) + geom_line(aes(x = start_year, y = uniq_ratio,  color=factor(Party),group = 1), alpha = 0.3, size = 2)+ geom_point(aes(x = start_year, y = uniq_ratio,  color=factor(Party),group = 1), size = 3) + geom_text(angle = 45, check_overlap = TRUE) + scale_color_manual(values=c("#0072B2", "#815283", "#548352","#afcf39","#fe3e0a","#ff2276")) + ggtitle("Unique Ratio - Higher the ratio, larger the vocabulary/not repeating content") + xlab("First Address Year") + ylab("Unique Ratio") + labs(color = "Political Party") + theme(text = element_text(size=20))
```

***Find the bi, tri, and four-grams in the speeches overall. Having difficulties with RWeka and rJava, thus resorted to quanteda. This corpus includes all addresses, whereas mine only includes the first one and addresses to Congress after assinations.***
```{r, cache = TRUE}
#Looking at all inaugural addresses and lengths
tokenInfo <- summary(data_corpus_inaugural)
#Similar plot below, but this is total words without taking out unneccesary words like I, as, or, and, etc.
ggplot(data=tokenInfo, aes(x=Year, y=Tokens, group=1, label = President)) + geom_line() + geom_point() + scale_x_discrete(labels=c(seq(1789,2012,12)), breaks=seq(1789,2012,12) ) + geom_text(angle = 45, check_overlap = TRUE) + ggtitle("Total Words in Inaugural Addresses")

#Find the bi-grams
bigrams <- dfm_trim(dfm(data_corpus_inaugural, tolower= TRUE, remove = stopwords("english"), removePunct = TRUE, removeSymbols = TRUE, removeNumbers = TRUE, stem = TRUE, ngrams = 2L, verbose = TRUE), min_count = 5)

#Find the bigrams with more than 10 occurances
which(colSums(bigrams) >= 10)

#How many times did these words occur? #Trump uses United States and unite only 3 times, whereas America is used 37 times
bigrams_matrix <- data.frame(bigrams[,which(colSums(bigrams) >= 10)])

bigram_freq <- data.frame(bigram_freq = colSums(data.frame(bigrams[,which(colSums(bigrams) >= 10)])))

#Who used the most of these bigrams?
sort(rowSums(data.frame(bigrams[,which(colSums(bigrams) >= 10)])), decreasing = TRUE)

# Favorite bigram
grams_pres <- data.frame(pres = rownames(bigrams_matrix))

grams_pres$fav_bigram <- c(colnames(bigrams_matrix[,2:ncol(bigrams_matrix)])[max.col(bigrams_matrix[,2:ncol(bigrams_matrix)],ties.method="first")])

# word cloud bigrams
wordcloud(words = rownames(bigram_freq), freq = bigram_freq$bigram_freq, min.freq = 10,
          max.words=500, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(6, "RdBu"))

#Find the tri-grams
trigrams <- dfm_trim(dfm(data_corpus_inaugural, tolower= TRUE, remove = stopwords("english"), removePunct = TRUE, removeSymbols = TRUE, removeNumbers = TRUE, stem = TRUE, ngrams = 3L, verbose = TRUE), min_count = 2)

#Find the trigrams with more than 2 occurances
which(colSums(trigrams) > 2)

#How many times did these words occur? 
colSums(data.frame(trigrams[,which(colSums(trigrams) > 2)]))

#Who used the most of these trigrams?
sort(rowSums(data.frame(trigrams[,which(colSums(trigrams) > 2)])), decreasing = TRUE)

#How many times did these words occur
trigrams_matrix <- data.frame(trigrams[,which(colSums(trigrams) >= 2)])

trigram_freq <- data.frame(bigram_freq = colSums(data.frame(trigrams[,which(colSums(trigrams) >= 2)])))

# Favorite Trigram
grams_pres$fav_trigram <- c(colnames(trigrams_matrix)[max.col(trigrams_matrix,ties.method="first")])

# Trigrams word cloud
wordcloud(words = rownames(trigram_freq), freq = trigram_freq$bigram_freq, min.freq = 2,
          max.words=500, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(7, "RdYlBu"))
```


***Perform topic modeling***
```{r, cache=TRUE} 
# I run LDA and use Gibbs sampling as our method for identifying the optimal parameters 
# Note: this make take some time to run (~10 mins)
set.seed(12345)
results <- LDA(tfm, k = 5, method = "Gibbs")

# Obtain the top w words (i.e., the w most probable words) for each topic, with the optional requirement that their probability is greater than thresh

#feel free to explore with different values of w and thresh
w=10
thresh = 0.005
set.seed(12345)
Terms <- terms(results, w,thresh) 
Terms
```

***Perform how the sentiment changes over time***
```{r, cache = TRUE}
dirname <- file.path("C:/Users/wjeer/OneDrive/Side Projects/Presidential Inagural Addresses/Addresses")
docs <- Corpus(DirSource(dirname, encoding = "UTF-8"))

docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)

# Convert all words to lowercase. 
docs <- tm_map(docs, content_transformer(tolower))

# Remove excess white spaces between words. 
docs <- tm_map(docs, stripWhitespace)

# Remove stopwords such as "a", "the", etc. 
docs <- tm_map(docs, removeWords, stopwords("english"))

# Find the sentiment of the document
washington_feels <- get_sentiment(get_tokens(docs[[1]]$content, pattern = "\\W"), method="syuzhet")
get_sentiment(get_tokens(docs[[1]]$content, pattern = "\\W"), method="syuzhet")

#Plot the distrubution of sentiment
ggplot() + geom_density(aes(x = washington_feels))

#Mean sentiment with stopwords is 0.0507763, sentiment increases to 0.1097948 for Washington using Syuzhet method
mean(get_sentiment(get_tokens(docs[[1]]$content, pattern = "\\W"), method="syuzhet"))

word_freq_dates <- word_freq_dates %>% arrange(start_year)
#Calculate sentiment for all presidents and add to the word_freq_dates dataframe
for (i in 1:42) {
  word_freq_dates$sentiment[i] <- mean(get_sentiment(get_tokens(docs[[i]]$content, pattern = "\\W"), method="bing"))
}

#Plot sentiment over time
ggplot(word_freq_dates, aes(x = start_year, y = sentiment,label = Last.Name)) + geom_line(aes(x = start_year, y = sentiment,  color=factor(Party),group = 1), alpha = 0.3, size = 2)+ geom_point(aes(x = start_year, y = sentiment,  color=factor(Party),group = 1), size = 3) + geom_text(angle = 45, check_overlap = TRUE)+ scale_color_manual(values=c("#0072B2", "#815283", "#548352","#afcf39","#fe3e0a","#ff2276")) + ggtitle("Inaugural Address Sentiment Over Time including First Messages to Congress after assassinations") + xlab("First Address Year")+ labs(color = "Political Party") + theme(text = element_text(size=20))
```

***Lastly, perhaps look at how speeches appear similar***
```{r}
#https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html

presDfm <- dfm(data_corpus_inaugural, tolower= TRUE, remove = stopwords("english"), removePunct = TRUE, removeSymbols = TRUE, removeNumbers = TRUE, stem = TRUE, verbose = TRUE)

presDfm <- dfm_trim(presDfm, min_count=5, min_docfreq=.25)
# hierarchical clustering - get distances on normalized dfm
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "tfidf")))
# hierarchical clustering the distance object
presCluster <- hclust(presDistMat)
# label with document names
presCluster$labels <- docnames(presDfm)
# plot as a dendrogram
plot(presCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
```


```{r}
#Find how similar Trumps Speech is to Clinton 1993
presDfm <- dfm(corpus_subset(data_corpus_inaugural), 
               remove = stopwords("english"),
               stem = TRUE, removePunct = TRUE, removeSymbols = TRUE, removeNumbers = TRUE)
trumpSimil <- textstat_simil(presDfm, c("2017-Trump" , "1997-Clinton"), n = NULL, 
                             margin = "documents", method = "cosine")
dotchart(as.list(trumpSimil)$"2017-Trump", xlab = "Cosine similarity")
```

